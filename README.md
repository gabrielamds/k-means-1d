# K-means 1D - Projeto de ProgramaÃ§Ã£o Concorrente e DistribuÃ­da

**ImplementaÃ§Ã£o completa do algoritmo K-means para dados unidimensionais com mÃºltiplos paradigmas de paralelizaÃ§Ã£o: Serial, OpenMP, CUDA, MPI e abordagens hÃ­bridas.**

---

## ğŸ“‹ SumÃ¡rio

- [VisÃ£o Geral](#visÃ£o-geral)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Requisitos](#requisitos)
- [InÃ­cio RÃ¡pido](#inÃ­cio-rÃ¡pido)
- [Uso Detalhado](#uso-detalhado)
- [Formato de Dados](#formato-de-dados)
- [ParÃ¢metros](#parÃ¢metros)
- [AnÃ¡lise de Resultados](#anÃ¡lise-de-resultados)
- [Resultados Esperados](#resultados-esperados)
- [Troubleshooting](#troubleshooting)
- [Contribuindo](#contribuindo)
- [ReferÃªncias](#referÃªncias)

---

## ğŸ¯ VisÃ£o Geral

Este projeto implementa o **algoritmo K-means 1D** (Lloyd, 1982) com foco em **anÃ¡lise comparativa de paralelizaÃ§Ã£o**. Trata-se de um trabalho acadÃªmico para a disciplina **ProgramaÃ§Ã£o Concorrente e DistribuÃ­da** que explora:

- **Baseline Serial**: ImplementaÃ§Ã£o sequencial para comparaÃ§Ã£o
- **OpenMP**: ParalelizaÃ§Ã£o em memÃ³ria compartilhada (multi-thread)
- **CUDA**: AceleraÃ§Ã£o em GPU (computaÃ§Ã£o massivamente paralela)
- **MPI**: ComputaÃ§Ã£o distribuÃ­da (multi-nÃ³)
- **HÃ­bridas**: CombinaÃ§Ãµes de paradigmas para mÃ¡xima eficiÃªncia

### Complexidade Temporal

| VersÃ£o | Complexidade | ObservaÃ§Ãµes |
|--------|-------------|-------------|
| Serial | O(qÂ·kÂ·n) | q=iteraÃ§Ãµes, k=clusters, n=pontos |
| OpenMP | O(qÂ·kÂ·n/t) | t=threads (speedup teÃ³rico) |
| CUDA | O(qÂ·kÂ·n/b) | TransferÃªncia GPU overhead |
| MPI | O(qÂ·kÂ·n/p) + comunicaÃ§Ã£o | p=processos, latÃªncia de rede |

---

## ğŸ“ Estrutura do Projeto

```
projeto-pcd-kmeans/
â”œâ”€â”€ serial/                      # Baseline: implementaÃ§Ã£o sequencial
â”‚   â”œâ”€â”€ Makefile
â”‚   â”œâ”€â”€ kmeans_1d_naive.c
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ openmp/                      # ParalelizaÃ§Ã£o com OpenMP
â”‚   â”œâ”€â”€ Makefile
â”‚   â”œâ”€â”€ kmeans_1d_omp.c
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ cuda/                        # AceleraÃ§Ã£o com CUDA
â”‚   â”œâ”€â”€ Makefile
â”‚   â”œâ”€â”€ kmeans_1d_cuda.cu
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ mpi/                         # ComputaÃ§Ã£o distribuÃ­da com MPI
â”‚   â”œâ”€â”€ Makefile
â”‚   â”œâ”€â”€ kmeans_1d_mpi.c
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ hybrid/                      # ImplementaÃ§Ãµes hÃ­bridas
â”‚   â”œâ”€â”€ Makefile
â”‚   â”œâ”€â”€ kmeans_1d_omp_cuda.cu    # OpenMP + CUDA
â”‚   â”œâ”€â”€ kmeans_1d_omp_mpi.c      # OpenMP + MPI
â”‚   â”œâ”€â”€ kmeans_1d_mpi_cuda.cu    # MPI + CUDA
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data/                        # GeraÃ§Ã£o e armazenamento de datasets
â”‚   â”œâ”€â”€ generate_data.py         # Script Python para gerar dados sintÃ©ticos
â”‚   â”œâ”€â”€ dados_pequeno.csv        # 10k pontos, 4 clusters
â”‚   â”œâ”€â”€ dados_medio.csv          # 100k pontos, 8 clusters
â”‚   â””â”€â”€ dados_grande.csv         # 1M pontos, 16 clusters
â”‚
â”œâ”€â”€ scripts/                     # UtilitÃ¡rios de benchmark
â”‚   â”œâ”€â”€ benchmark_all.sh         # Executa todos os benchmarks
â”‚   â”œâ”€â”€ benchmark_serial.sh
â”‚   â”œâ”€â”€ benchmark_openmp.sh
â”‚   â”œâ”€â”€ benchmark_cuda.sh
â”‚   â”œâ”€â”€ benchmark_mpi.sh
â”‚   â””â”€â”€ benchmark_hybrid.sh
â”‚
â”œâ”€â”€ analysis/                    # AnÃ¡lise e visualizaÃ§Ã£o
â”‚   â”œâ”€â”€ analyze_results.py       # Script Python para anÃ¡lise
â”‚   â”œâ”€â”€ plot_speedup.py
â”‚   â”œâ”€â”€ plot_scaling.py
â”‚   â””â”€â”€ plot_efficiency.py
â”‚
â”œâ”€â”€ report/                      # SaÃ­da: grÃ¡ficos e relatÃ³rios
â”‚   â”œâ”€â”€ figures/                 # GrÃ¡ficos gerados
â”‚   â”‚   â”œâ”€â”€ speedup_*.png
â”‚   â”‚   â”œâ”€â”€ scaling_*.png
â”‚   â”‚   â””â”€â”€ efficiency_*.png
â”‚   â””â”€â”€ RESULTS.md               # AnÃ¡lise detalhada
â”‚
â”œâ”€â”€ results/                     # Dados brutos de benchmark
â”‚   â”œâ”€â”€ serial_results.csv
â”‚   â”œâ”€â”€ openmp_results.csv
â”‚   â”œâ”€â”€ cuda_results.csv
â”‚   â”œâ”€â”€ mpi_results.csv
â”‚   â””â”€â”€ hybrid_results.csv
â”‚
â”œâ”€â”€ Makefile                     # OrquestraÃ§Ã£o geral
â”œâ”€â”€ README.md                    # Este arquivo
â””â”€â”€ LICENSE                      # LicenÃ§a do projeto
```

---

## ğŸ“¦ Requisitos

### Compiladores e Ferramentas

```bash
# GCC 7.0+ com C99 (obrigatÃ³rio)
gcc --version

# OpenMP (geralmente incluÃ­do no GCC)
gcc -fopenmp --version

# CUDA Toolkit 11.0+ (opcional, para GPU)
nvcc --version
nvidia-smi

# Open MPI 4.0+ (opcional, para distribuÃ­do)
mpicc --version
mpirun --version

# Python 3.8+ (opcional, para anÃ¡lises)
python3 --version
pip3 install matplotlib numpy
```

### VerificaÃ§Ã£o RÃ¡pida

```bash
# Rodar script de validaÃ§Ã£o
chmod +x scripts/check_setup.sh
./scripts/check_setup.sh
```

### InstalaÃ§Ã£o de DependÃªncias

#### Ubuntu/Debian
```bash
# GCC e OpenMP
sudo apt-get install build-essential gcc g++ gomp

# OpenMP dev (se necessÃ¡rio)
sudo apt-get install libomp-dev

# Open MPI
sudo apt-get install openmpi-bin libopenmpi-dev

# Python
sudo apt-get install python3 python3-pip
pip3 install matplotlib numpy scipy
```

#### CentOS/RHEL
```bash
# GCC com OpenMP
sudo yum install gcc gcc-c++ gomp

# Open MPI
sudo yum install openmpi openmpi-devel
export PATH=$PATH:/usr/lib64/openmpi/bin

# Python
sudo yum install python3 python3-pip
pip3 install matplotlib numpy scipy
```

#### macOS (Homebrew)
```bash
# GCC com OpenMP
brew install gcc

# Open MPI
brew install open-mpi

# Python
brew install python3
pip3 install matplotlib numpy scipy
```

---

## ğŸš€ InÃ­cio RÃ¡pido

### 1ï¸âƒ£ Clonar e Entrar no DiretÃ³rio

```bash
cd projeto-pcd-kmeans
```

### 2ï¸âƒ£ Compilar Todas as VersÃµes DisponÃ­veis

```bash
make compile-all
```

Isso detecta automaticamente qual compilador estÃ¡ disponÃ­vel e compila apenas as versÃµes suportadas.

### 3ï¸âƒ£ Gerar Dados de Teste

```bash
make data
```

Gera datasets sintÃ©ticos em `data/`:
- `dados_pequeno.csv`: 10k pontos, 4 clusters
- `dados_medio.csv`: 100k pontos, 8 clusters
- `dados_grande.csv`: 1M pontos, 16 clusters

### 4ï¸âƒ£ Executar um Exemplo RÃ¡pido

```bash
cd serial
make
./kmeans_1d_naive ../data/dados_pequeno.csv ../data/dados_pequeno_centroides.csv 50 1e-6
```

### 5ï¸âƒ£ Rodar Todos os Benchmarks

```bash
make benchmark-all
```

Resulta em `results/*.csv` com timings.

### 6ï¸âƒ£ Gerar AnÃ¡lises e GrÃ¡ficos

```bash
make analyze
```

Produz grÃ¡ficos em `report/figures/` (requer Python).

---

## ğŸ”§ Uso Detalhado

### Serial (Baseline)

```bash
cd serial && make

./kmeans_1d_naive <dados.csv> <centroides_init.csv> <max_iter> <eps>
```

**Exemplo:**
```bash
./kmeans_1d_naive ../data/dados_medio.csv ../data/dados_medio_centroides_init.csv 50 1e-6
```

**SaÃ­da:**
```
IteraÃ§Ã£o 1/50: SSE = 45321.23, Î” = 1.000000
IteraÃ§Ã£o 2/50: SSE = 22451.12, Î” = 0.505682
...
ConvergÃªncia em iteraÃ§Ã£o 18
Tempo total: 0.342s
```

### OpenMP (Multi-thread)

```bash
cd openmp && make

./kmeans_1d_omp <dados.csv> <centroides_init.csv> <max_iter> <eps> [threads] [schedule] [chunk]
```

**ParÃ¢metros OpenMP:**
- `threads`: NÃºmero de threads (0 = detectar automaticamente)
- `schedule`: EstratÃ©gia de divisÃ£o (`static`, `dynamic`, `guided`)
- `chunk`: Tamanho do chunk (0 = automÃ¡tico)

**Exemplo com 8 threads e static scheduling:**
```bash
./kmeans_1d_omp ../data/dados_medio.csv ../data/dados_medio_centroides_init.csv 50 1e-6 8 static 0
```

**Esperado:**
```
Threads OpenMP: 8
Scheduling: static
IteraÃ§Ã£o 1/50: SSE = 45321.23, Î” = 1.000000 [tempo par: 0.012s]
...
Speedup: ~7-8x vs serial
```

### CUDA (GPU)

```bash
cd cuda && make

./kmeans_1d_cuda <dados.csv> <centroides_init.csv> <max_iter> <eps> [block_size]
```

**ParÃ¢metros CUDA:**
- `block_size`: Threads por bloco (128, 256, 512)

**Exemplo com 256 threads/bloco:**
```bash
./kmeans_1d_cuda ../data/dados_grande.csv ../data/dados_grande_centroides_init.csv 50 1e-6 256
```

**Esperado:**
```
GPU: NVIDIA ...
Block size: 256
MemÃ³ria GPU: ... MB
IteraÃ§Ã£o 1/50: SSE = 45321.23, Î” = 1.000000
Tempo H2D (cÃ³pia hostâ†’device): 0.045s
Tempo D2H (cÃ³pia deviceâ†’host): 0.051s
Speedup: ~10-12x vs serial (dataset grande)
```

### MPI (DistribuÃ­do)

```bash
cd mpi && make

mpirun -np <num_processos> ./kmeans_1d_mpi <dados.csv> <centroides_init.csv> <max_iter> <eps>
```

**Exemplo com 4 processos:**
```bash
mpirun -np 4 ./kmeans_1d_mpi ../data/dados_grande.csv ../data/dados_grande_centroides_init.csv 50 1e-6
```

**Esperado:**
```
MPI processos: 4
Rank 0: Lendo dados...
Rank 1,2,3: Distribuindo chunk...
IteraÃ§Ã£o 1/50: SSE = 45321.23
SincronizaÃ§Ã£o inter-processos: 0.003s
Speedup: ~4x vs serial (overhead de comunicaÃ§Ã£o)
```

### HÃ­bridos

#### OpenMP + CUDA

```bash
cd hybrid && make

./kmeans_1d_omp_cuda <dados.csv> <centroides_init.csv> <max_iter> <eps> <threads> <block_size>
```

**Exemplo:** 4 threads OpenMP + 256 threads CUDA/bloco
```bash
./kmeans_1d_omp_cuda ../data/dados_medio.csv ../data/dados_medio_centroides_init.csv 50 1e-6 4 256
```

#### OpenMP + MPI

```bash
mpirun -np 2 ./kmeans_1d_omp_mpi <dados.csv> <centroides_init.csv> <max_iter> <eps> [threads]
```

**Exemplo:** 2 processos MPI + 4 threads/processo
```bash
mpirun -np 2 ./kmeans_1d_omp_mpi ../data/dados_grande.csv ../data/dados_grande_centroides_init.csv 50 1e-6 4
```

#### MPI + CUDA

```bash
mpirun -np 4 ./kmeans_1d_mpi_cuda <dados.csv> <centroides_init.csv> <max_iter> <eps> [block_size]
```

**Exemplo:** 4 processos MPI, cada com GPU
```bash
mpirun -np 4 ./kmeans_1d_mpi_cuda ../data/dados_grande.csv ../data/dados_grande_centroides_init.csv 50 1e-6 256
```

---

## ğŸ“„ Formato de Dados

### Arquivo CSV de Dados

**Requisitos:**
- **Sem cabeÃ§alho**
- **Um valor por linha** (1D)
- Delimitadores suportados: vÃ­rgula, ponto-e-vÃ­rgula, espaÃ§o, tabulaÃ§Ã£o
- Valores numÃ©ricos (float ou int)

**Exemplo (`dados.csv`):**
```
10.5
23.1
15.7
8.2
19.4
...
```

### Arquivo CSV de CentrÃ³ides Iniciais

**Requisitos:**
- **Sem cabeÃ§alho**
- **Um centrÃ³ide por linha** (valores iniciais)
- Mesmo delimitador que dados

**Exemplo (`centroides_init.csv`):**
```
10.0
20.0
15.0
5.0
```

### Gerar Dados SintÃ©ticos

```bash
cd data
python3 generate_data.py --points 100000 --clusters 8 --output dados_custom.csv --seed 42
```

**OpÃ§Ãµes:**
- `--points`: NÃºmero de pontos (padrÃ£o: 100000)
- `--clusters`: NÃºmero de clusters (padrÃ£o: 8)
- `--output`: Arquivo de saÃ­da (padrÃ£o: dados.csv)
- `--seed`: Seed para reprodutibilidade (padrÃ£o: 42)
- `--range`: Intervalo de valores (padrÃ£o: 0-1000)

---

## âš™ï¸ ParÃ¢metros

### ParÃ¢metros Comuns

| ParÃ¢metro | DescriÃ§Ã£o | Tipo | PadrÃ£o | Intervalo |
|-----------|-----------|------|--------|-----------|
| `max_iter` | MÃ¡ximo de iteraÃ§Ãµes | int | 50 | 1-1000 |
| `eps` | CritÃ©rio de convergÃªncia (variaÃ§Ã£o relativa SSE) | float | 1e-4 | 1e-8 a 1e-2 |

### ParÃ¢metros OpenMP

| ParÃ¢metro | DescriÃ§Ã£o | Tipo | PadrÃ£o | OpÃ§Ãµes |
|-----------|-----------|------|--------|--------|
| `threads` | NÃºmero de threads | int | 0 (auto) | 1-64 |
| `schedule` | DivisÃ£o de trabalho | string | static | `static`, `dynamic`, `guided` |
| `chunk` | Tamanho do bloco | int | 0 (auto) | 0-1000 |

**RecomendaÃ§Ãµes de scheduling:**
- `static`: Balanceamento predefinido, baixo overhead (bom para loops regulares)
- `dynamic`: Balanceamento runtime, alto overhead (bom para carga desigual)
- `guided`: HÃ­brido (chunks grandes no inÃ­cio, pequenos no fim)

### ParÃ¢metros CUDA

| ParÃ¢metro | DescriÃ§Ã£o | Tipo | PadrÃ£o | Recomendado |
|-----------|-----------|------|--------|-----------|
| `block_size` | Threads por bloco | int | 256 | 128-512 |

**Trade-offs de block_size:**
- **128**: Menos occupancy, mais registros livres
- **256**: BalanÃ§o (recomendado)
- **512**: MÃ¡xima occupancy, menos registros/thread

### ParÃ¢metros MPI

| ParÃ¢metro | DescriÃ§Ã£o | Tipo | Recomendado |
|-----------|-----------|------|-----------|
| `-np` | NÃºmero de processos | int | â‰¤ nÃ³s da mÃ¡quina |

---

## ğŸ“Š AnÃ¡lise de Resultados

### Scripts de AnÃ¡lise

```bash
make analyze  # Roda todas as anÃ¡lises
```

Ou individualmente:

```bash
cd analysis

# GrÃ¡ficos de speedup
python3 plot_speedup.py

# Escalabilidade (strong/weak scaling)
python3 plot_scaling.py

# EficiÃªncia paralela
python3 plot_efficiency.py

# RelatÃ³rio completo
python3 analyze_results.py
```

### Outputs Gerados

1. **Speedup** (`report/figures/speedup_*.png`)
   - GrÃ¡fico: Speedup vs nÃºmero de recursos
   - Compara todas as versÃµes paralelizadas vs serial
   - Mostra limite teÃ³rico (Lei de Amdahl)

2. **Escalabilidade** (`report/figures/*_scaling.png`)
   - Strong scaling: Problema fixo, aumentar recursos
   - Weak scaling: Aumentar problema e recursos proporcionalmente
   - Identifica ponto de saturaÃ§Ã£o

3. **EficiÃªncia** (`report/figures/efficiency_*.png`)
   - EficiÃªncia paralela: Speedup / num_recursos
   - Esperado: 70-90% para bom scaling

4. **RelatÃ³rio Completo** (`report/RESULTS.md`)
   - AnÃ¡lise estatÃ­stica
   - RecomendaÃ§Ãµes de uso
   - Trade-offs

### InterpretaÃ§Ã£o de Resultados

```
Speedup Linear (ideal):     S(p) = p  â†’  EficiÃªncia = 100%
Speedup Sublinear (real):   S(p) < p  â†’  EficiÃªncia < 100% (overhead)
Lei de Amdahl:              S(p) = 1 / [f + (1-f)/p]
                            (f = fraÃ§Ã£o serial)
```

---

## ğŸ“ˆ Resultados Esperados

### ComparaÃ§Ã£o de Speedup vs Serial

| VersÃ£o | Dataset Pequeno (10k) | Dataset MÃ©dio (100k) | Dataset Grande (1M) |
|--------|---------------------|----------------------|-------------------|
| **Serial** | 1x (baseline) | 1x (baseline) | 1x (baseline) |
| **OpenMP (8 threads)** | 6-7x | 7-8x | 7-8x |
| **CUDA (256 blocks)** | 5-8x* | 10-12x | 12-15x |
| **MPI (4 processos)** | 2-3x** | 3.5-4x | 4-4.5x |
| **OpenMP + CUDA** | 8-10x | 12-14x | 15-18x |
| **OpenMP + MPI** | 6-8x | 8-10x | 10-12x |
| **MPI + CUDA** | 10-12x | 15-18x | 18-22x |

**ObservaÃ§Ãµes:**
- *Dataset pequeno: overhead CUDA domina (requer â‰¥100k pontos)
- **MPI tem overhead de comunicaÃ§Ã£o; melhor em datasets massivos

### Quando Usar Cada Paradigma

| Paradigma | Caso de Uso | Dataset | Hardware |
|-----------|-----------|---------|----------|
| **Serial** | Debugging, baseline, teÃ³rico | Qualquer | CPU |
| **OpenMP** | Workstation multi-core, compartilhado | MÃ©dio-Grande | CPU multi-core |
| **CUDA** | GPU disponÃ­vel, cÃ¡lculo intensivo | Grande (>100k) | NVIDIA GPU |
| **MPI** | Cluster distribuÃ­do, problema massivo | Massivo (>10M) | Multi-nÃ³ |
| **OpenMP+CUDA** | Workstation forte, mÃ¡xima performance | Grande | CPU + GPU |
| **OpenMP+MPI** | Cluster com nÃ³s multi-core | Massivo | Multi-nÃ³ multi-core |
| **MPI+CUDA** | Sistema HPC, mÃºltiplas GPUs | Massivo | Multi-GPU cluster |

---

## ğŸ” Troubleshooting

### âŒ CompilaÃ§Ã£o GCC falha

**Erro:** `gcc: command not found`

**SoluÃ§Ã£o:**
```bash
# Instalar GCC
sudo apt-get install gcc  # Ubuntu/Debian
sudo yum install gcc      # CentOS/RHEL
brew install gcc          # macOS

# Verificar
gcc --version
```

---

### âŒ OpenMP nÃ£o encontrado

**Erro:** `-fopenmp: unrecognized command line option`

**SoluÃ§Ã£o:**
```bash
# Instalar OpenMP (geralmente com GCC)
gcc --version  # Se GCC 7+, OpenMP deve estar incluÃ­do

# Alternativa (libgomp)
sudo apt-get install libgomp1

# Ou compilar sem OpenMP
make ENABLE_OPENMP=0
```

---

### âŒ CUDA nÃ£o instalado

**Erro:** `nvcc: command not found`

**SoluÃ§Ã£o:**
```bash
# Baixar CUDA Toolkit 11.0+ de https://developer.nvidia.com/cuda-toolkit
# Instalar seguindo o guia oficial

# Verificar apÃ³s instalaÃ§Ã£o
nvcc --version
nvidia-smi

# Se nÃ£o encontrar, adicionar ao PATH
export PATH=$PATH:/usr/local/cuda/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64
```

---

### âŒ MPI nÃ£o encontrado

**Erro:** `mpicc: command not found`

**SoluÃ§Ã£o:**
```bash
# Ubuntu/Debian
sudo apt-get install openmpi-bin libopenmpi-dev

# CentOS/RHEL
sudo yum install openmpi openmpi-devel
export PATH=$PATH:/usr/lib64/openmpi/bin

# Verificar
mpicc --version
mpirun --version
```

---

### âŒ Erro de memÃ³ria GPU

**Erro:** `CUDA error: out of memory`

**SoluÃ§Ã£o:**
```bash
# Usar dataset menor
./kmeans_1d_cuda data/dados_pequeno.csv centroides.csv 50 1e-6 256

# Ou reduzir block_size
./kmeans_1d_cuda data/dados_medio.csv centroides.csv 50 1e-6 128

# Verificar memÃ³ria GPU disponÃ­vel
nvidia-smi

# Limpar GPU
nvidia-smi --query-gpu=memory.free --format=csv
```

---

### âŒ Python nÃ£o encontrado (anÃ¡lises)

**Erro:** `ModuleNotFoundError: No module named 'matplotlib'`

**SoluÃ§Ã£o:**
```bash
# Instalar Python 3.8+
python3 --version

# Instalar dependÃªncias
pip3 install matplotlib numpy scipy

# Ou usar requirements.txt (se existir)
pip3 install -r requirements.txt
```

---

### âŒ MPI falha em executar

**Erro:** `No hosts file was found` ou problema de conectividade

**SoluÃ§Ã£o:**
```bash
# Para execuÃ§Ã£o local (sem cluster real)
mpirun --allow-run-as-root -np 4 ./kmeans_1d_mpi ...

# Ou especificar localhost
mpirun -H localhost -np 4 ./kmeans_1d_mpi ...
```

---

### âŒ Makefile nÃ£o funciona

**Erro:** `make: *** No targets specified. Stop.`

**SoluÃ§Ã£o:**
```bash
# Verificar Makefile existe
ls -la Makefile

# Executar alvo especÃ­fico
make compile-all
make benchmark-all

# ForÃ§ar recompilaÃ§Ã£o
make clean
make compile-all
```

---

## ğŸ§¹ Limpeza

```bash
# Remove executÃ¡veis compilados
make clean

# Remove executÃ¡veis + dados gerados
make distclean

# Remove tudo (executÃ¡veis, dados, resultados, grÃ¡ficos)
make full-clean
```

---

## ğŸ¤ Contribuindo

Para adicionar novas implementaÃ§Ãµes ou otimizaÃ§Ãµes:

### Passos

1. **Crie um diretÃ³rio** com nome descritivo
   ```bash
   mkdir new_paradigm/
   cd new_paradigm/
   ```

2. **Adicione Makefile** seguindo padrÃ£o do projeto
   ```makefile
   CC = gcc
   CFLAGS = -O3 -std=c99
   
   all: kmeans_1d_new
   
   kmeans_1d_new: kmeans_1d_new.c
   	$(CC) $(CFLAGS) -o kmeans_1d_new kmeans_1d_new.c
   
   clean:
   	rm -f kmeans_1d_new
   ```

3. **Mantenha interface CLI consistente**
   - Argumentos: `<dados.csv> <centroides.csv> <max_iter> <eps> [params...]`
   - SaÃ­da: SSE por iteraÃ§Ã£o em formato consistente

4. **Adicione README.md** documentando particularidades

5. **Crie script de benchmark** em `scripts/benchmark_<paradigm>.sh`

6. **Atualize Makefile raiz** e este README

### Template de README para novo paradigma

```markdown
# K-means 1D - [Nome do Paradigma]

## DescriÃ§Ã£o
[Breve descriÃ§Ã£o tÃ©cnica]

## CompilaÃ§Ã£o
[InstruÃ§Ãµes especÃ­ficas]

## Uso
[Exemplos e parÃ¢metros]

## Requisitos
[DependÃªncias especÃ­ficas]

## Notas de ImplementaÃ§Ã£o
[DecisÃµes de design, trade-offs]
```

---

## ğŸ“š ReferÃªncias

### Artigos CientÃ­ficos

1. **Lloyd, S.** (1982). "Least squares quantization in PCM." IEEE Transactions on Information Theory, 28(2), 129-137.
   - Artigo original do algoritmo K-means

2. **Wang, H., et al.** (2011). "Ckmeans.1d.dp: Optimal k-means Clustering in One Dimension by Dynamic Programming." The R Journal, 3(2), 29-33.
   - AnÃ¡lise aprofundada de K-means 1D e otimizaÃ§Ãµes

3. **OpenMP Architecture Review Board** (2021). "OpenMP API Specification 5.1."
   - EspecificaÃ§Ã£o completa de OpenMP

4. **NVIDIA Corporation** (2023). "CUDA C Programming Guide."
   - Guia oficial de programaÃ§Ã£o CUDA

5. **Message Passing Interface Forum** (2021). "MPI: A Message-Passing Interface Standard 3.1."
   - EspecificaÃ§Ã£o MPI completa

### Recursos Online

- [GCC Compiler](https://gcc.gnu.org/)
- [OpenMP Official](https://www.openmp.org/)
- [CUDA Toolkit Download](https://developer.nvidia.com/cuda-toolkit)
- [Open MPI Project](https://www.open-mpi.org/)
- [Python Scientific Stack](https://www.scipy.org/)

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© material educacional para a disciplina **ProgramaÃ§Ã£o Concorrente e DistribuÃ­da** da [InstituiÃ§Ã£o/Universidade].

VocÃª Ã© livre para estudar, modificar e distribuir este cÃ³digo para fins educacionais. Para uso comercial, consulte a licenÃ§a completa em `LICENSE`.

---

## ğŸ‘¥ Autores

Desenvolvido como trabalho acadÃªmico em ProgramaÃ§Ã£o Concorrente e DistribuÃ­da.

**Contribuidores:**
- [Seu Nome] - [Papel/Paradigma]

---

## ğŸ“ Notas Finais

### Dicas de Uso

1. **Comece pelo serial** para entender o baseline
2. **Use datasets pequenos** para debug, grandes para anÃ¡lise final
3. **Mantenha consistÃªncia** de convergÃªncia entre versÃµes (validate!)
4. **Documente seus temings** com `time` ou `perf`
5. **Reproduza resultados** com seeds fixas

### PrÃ³ximos Passos

- [ ] Implementar validaÃ§Ã£o de resultados (checksum dos centrÃ³ides)
- [ ] Adicionar profiling com `gprof` ou `perf`
- [ ] Estender para K-means 2D/3D
- [ ] Integrar com bibliotecas (OpenCL, TensorFlow)
- [ ] Criar documentaÃ§Ã£o tÃ©cnica detalhada
- [ ] Adicionar testes unitÃ¡rios

---

**Ãšltima atualizaÃ§Ã£o:** Dezembro 2025
**Status do Projeto:** Ativo (desenvolvimento educacional)
